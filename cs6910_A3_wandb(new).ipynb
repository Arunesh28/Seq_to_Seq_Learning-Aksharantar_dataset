{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from io import open\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import wandb\n",
    "import re\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.chr2index = {}\n",
    "        self.chr2count = {}\n",
    "        self.index2chr = {0: \"$\", 1: \"#\"}\n",
    "        self.n_chrs = 2 \n",
    "\n",
    "    def addword(self, word):\n",
    "        for ch in word:\n",
    "            self.addchr(ch)\n",
    "\n",
    "    def addchr(self, ch):\n",
    "        if ch not in self.chr2index:\n",
    "            self.chr2index[ch] = self.n_chrs\n",
    "            self.chr2count[ch] = 1\n",
    "            self.index2chr[self.n_chrs] = ch\n",
    "            self.n_chrs += 1\n",
    "        else:\n",
    "            self.chr2count[ch] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 25\n",
    "# def unicodeToAscii(s):\n",
    "#     return ''.join(\n",
    "#         c for c in unicodedata.normalize('NFD', s)\n",
    "#         if unicodedata.category(c) != 'Mn'\n",
    "#     )\n",
    "\n",
    "\n",
    "# def normalizeString(s):\n",
    "#     s = unicodeToAscii(s.strip())\n",
    "#     s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "#     s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "#     return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def readLangs(lang1,lang2):\n",
    "    pairs = ()\n",
    "    for i in ['train','valid','test']:\n",
    "        fd = open('aksharantar_sampled/'+lang2+'/'+lang2+'_'+i+'.csv')\n",
    "        lines = fd.read().strip().split('\\n')\n",
    "        pairs += ([[s for s in l.split(',')] for l in lines],)\n",
    "    return pairs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lang = Lang(\"eng\")\n",
    "output_lang = Lang(\"mni\")\n",
    "train_pairs,valid_pairs,test_pairs = readLangs(\"eng\",\"mni\")\n",
    "\n",
    "for pair in train_pairs:\n",
    "    input_lang.addword(pair[0])\n",
    "    output_lang.addword(pair[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromWord(lang, word):\n",
    "    return [lang.chr2index[ch] for ch in word]\n",
    "\n",
    "\n",
    "def tensorFromWord(lang,word):\n",
    "    indexes = indexesFromWord(lang, word)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromWord(input_lang, pair[0])\n",
    "    target_tensor = tensorFromWord(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_layer_size, num_encoder_layers, cell_type, dropout_prob, bidirectional):\n",
    "      super(EncoderRNN, self).__init__()\n",
    "      self.input_size = input_size\n",
    "      self.hidden_layer_size = hidden_layer_size\n",
    "      self.num_encoder_layers = num_encoder_layers\n",
    "      self.cell_type = cell_type\n",
    "\n",
    "      self.embedding = nn.Embedding(self.input_size, embedding_size)\n",
    "\n",
    "      if cell_type == 'RNN':\n",
    "        self.rnn = nn.RNN(input_size = embedding_size, hidden_size = hidden_layer_size, num_layers = num_encoder_layers, dropout = dropout_prob, bidirectional = bidirectional)\n",
    "      elif cell_type == 'LSTM':\n",
    "        self.rnn = nn.LSTM(input_size = embedding_size, hidden_size = hidden_layer_size, num_layers = num_encoder_layers, dropout = dropout_prob, bidirectional = bidirectional)\n",
    "      elif cell_type == 'GRU':\n",
    "        self.rnn = nn.GRU(input_size = embedding_size, hidden_size = hidden_layer_size, num_layers = num_encoder_layers, dropout = dropout_prob, bidirectional = bidirectional)\n",
    "      self.dropout = nn.Dropout(dropout_prob)\n",
    "      self.D = 1\n",
    "      if bidirectional == True :\n",
    "        self.D = 2\n",
    "\n",
    "    def forward(self, input_tensor, prev_hidden, prev_cell = None):\n",
    "      embedded = self.embedding(input_tensor).view(1,1,-1)\n",
    "      embedded = self.dropout(embedded)\n",
    "      if self.cell_type == 'RNN':\n",
    "        output, hidden = self.rnn(embedded,prev_hidden)\n",
    "      elif self.cell_type == 'LSTM':\n",
    "        output, (hidden,cell) = self.rnn(embedded,(prev_hidden,prev_cell))\n",
    "        return output, (hidden,cell)\n",
    "      elif self.cell_type == 'GRU':\n",
    "        output, hidden = self.rnn(embedded,prev_hidden)\n",
    "        \n",
    "      return output,hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "      if self.cell_type == 'LSTM':\n",
    "        hidden = torch.zeros(self.D*self.num_encoder_layers,1,self.hidden_layer_size,device = device)\n",
    "        cell = torch.zeros(self.D*self.num_encoder_layers,1,self.hidden_layer_size,device = device)\n",
    "        return hidden,cell\n",
    "      else :\n",
    "        hidden = torch.zeros(self.D*self.num_encoder_layers,1,self.hidden_layer_size,device = device)\n",
    "      return hidden\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DECODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Decoder\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "  def __init__(self, output_size, embedding_size, hidden_layer_size, num_decoder_layers, cell_type, dropout_prob, bidirectional):\n",
    "    super(DecoderRNN, self).__init__()\n",
    "    self.output_size = output_size\n",
    "    self.hidden_layer_size = hidden_layer_size\n",
    "    self.num_decoder_layers = num_decoder_layers\n",
    "    self.cell_type = cell_type\n",
    "    self.embedding_size = embedding_size\n",
    "    self.embedding = nn.Embedding(output_size, embedding_size)\n",
    "   \n",
    "    if cell_type == 'RNN':\n",
    "      self.rnn = nn.RNN(input_size = embedding_size, hidden_size = hidden_layer_size, num_layers = num_decoder_layers, dropout = dropout_prob, bidirectional = bidirectional)\n",
    "    elif cell_type == 'LSTM':\n",
    "      self.rnn = nn.LSTM(input_size = embedding_size, hidden_size = hidden_layer_size, num_layers = num_decoder_layers, dropout = dropout_prob, bidirectional = bidirectional)\n",
    "    elif cell_type == 'GRU':\n",
    "      self.rnn = nn.GRU(input_size = embedding_size, hidden_size = hidden_layer_size, num_layers = num_decoder_layers, dropout = dropout_prob, bidirectional = bidirectional)\n",
    "      \n",
    "    self.D = 1\n",
    "    if bidirectional == True :\n",
    "      self.D = 2\n",
    "      \n",
    "    self.dropout = nn.Dropout(dropout_prob)\n",
    "    self.out = nn.Linear(self.D*hidden_layer_size, output_size)\n",
    "    self.softmax = nn.LogSoftmax(dim = 1)\n",
    "      \n",
    "  def forward(self, input_tensor, prev_hidden, prev_cell = None):\n",
    "    embedded = self.embedding(input_tensor).view(1,1,-1)\n",
    "    embedded = F.relu(embedded)\n",
    "    embedded = self.dropout(embedded)\n",
    "    \n",
    "    if self.cell_type == 'RNN':\n",
    "      output, hidden = self.rnn(embedded,prev_hidden)\n",
    "      \n",
    "    elif self.cell_type == 'LSTM':\n",
    "      output,(hidden,cell) = self.rnn(embedded,(prev_hidden,prev_cell))\n",
    "    \n",
    "    elif self.cell_type == 'GRU':\n",
    "      output, hidden = self.rnn(embedded,prev_hidden)\n",
    "\n",
    "    output = self.softmax(self.out(output[0]))\n",
    "\n",
    "    if self.cell_type == 'LSTM':\n",
    "      return output,(hidden,cell)\n",
    "    \n",
    "    return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor,target_tensor,encoder,decoder,encoder_optimizer,decoder_optimizer,criterion,max_length = MAX_LENGTH,teacher_forcing_ratio = 0.5):\n",
    "    \n",
    "    if encoder.cell_type == 'LSTM':\n",
    "      encoder_hidden,encoder_cell = encoder.initHidden()\n",
    "    else :\n",
    "      encoder_hidden = encoder.initHidden()\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    encoder_outputs = torch.zeros(max_length, encoder.D*encoder.hidden_layer_size, device=device)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    for ei in range(input_length):\n",
    "      if encoder.cell_type == 'LSTM':\n",
    "        encoder_output,(encoder_hidden,encoder_cell) = encoder(input_tensor = input_tensor[ei],prev_hidden = encoder_hidden,prev_cell = encoder_cell)\n",
    "      else :\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor = input_tensor[ei], prev_hidden = encoder_hidden)\n",
    "      # print(encoder_output.size())\n",
    "      encoder_outputs[ei] = encoder_output[0][0]\n",
    "      \n",
    "    decoder_input = torch.tensor([[SOS_token]],device=device)\n",
    "    # decoder_outputs = torch.zeros(target_length)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    if decoder.cell_type == 'LSTM':\n",
    "      decoder_cell = encoder_cell\n",
    "    \n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing :\n",
    "      for di in range(target_length):\n",
    "        if decoder.cell_type == 'LSTM':\n",
    "          decoder_output,(decoder_hidden,decoder_cell) = decoder(input_tensor = decoder_input,prev_hidden = decoder_hidden,prev_cell = decoder_cell)\n",
    "        else:\n",
    "          decoder_output,decoder_hidden = decoder(decoder_input,decoder_hidden)\n",
    "        loss += criterion(decoder_output, target_tensor[di])\n",
    "        decoder_input = target_tensor[di]\n",
    "\n",
    "    else :\n",
    "      for di in range(target_length):\n",
    "        if decoder.cell_type == 'LSTM':\n",
    "          decoder_output,(decoder_hidden,decoder_cell) = decoder(input_tensor = decoder_input,prev_hidden = decoder_hidden,prev_cell = decoder_cell)\n",
    "        else:\n",
    "          decoder_output,decoder_hidden = decoder(decoder_input,decoder_hidden)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()\n",
    "        loss += criterion(decoder_output, target_tensor[di])\n",
    "        if decoder_input.item() == EOS_token:\n",
    "          break\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder,decoder,encoder_optimizer,decoder_optimizer,criterion):\n",
    "    \n",
    "    training_pairs = [tensorsFromPair(pair) for pair in train_pairs]\n",
    "    training_loss = 0\n",
    "    it = 0\n",
    "    for pair in training_pairs:\n",
    "        input_tensor = pair[0]\n",
    "        target_tensor = pair[1]\n",
    "        loss = train(input_tensor,target_tensor,encoder,decoder,encoder_optimizer,decoder_optimizer,criterion)\n",
    "        training_loss += loss\n",
    "        if it%200==0:\n",
    "            print(it,\"done\")\n",
    "        it += 1\n",
    "    return training_loss/len(training_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder,decoder,word,target = None,criterion = None,max_length = MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromWord(input_lang,word)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        target_length = 0\n",
    "        if target != None:\n",
    "            target_tensor = tensorFromWord(output_lang,target)\n",
    "            target_length = target_tensor.size(0)\n",
    "        \n",
    "        loss = 0\n",
    "        \n",
    "        if encoder.cell_type == 'LSTM':\n",
    "            encoder_hidden,encoder_cell = encoder.initHidden()\n",
    "        else :\n",
    "            encoder_hidden = encoder.initHidden()\n",
    "            \n",
    "        encoder_outputs = torch.zeros(max_length,encoder.D*encoder.hidden_layer_size,device=device)\n",
    "        \n",
    "        for ei in range(input_length):\n",
    "            if encoder.cell_type == 'LSTM':\n",
    "                encoder_output,(encoder_hidden,encoder_cell) = encoder(input_tensor = input_tensor[ei],prev_hidden = encoder_hidden,prev_cell = encoder_cell)\n",
    "            else :\n",
    "                encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "        \n",
    "        decoder_input = torch.tensor([[SOS_token]],device=device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        if decoder.cell_type == 'LSTM':\n",
    "            decoder_cell = encoder_cell\n",
    "            \n",
    "        decoded_word = ''\n",
    "        \n",
    "\n",
    "        for di in range(max_length):\n",
    "            if decoder.cell_type == 'LSTM':\n",
    "                decoder_output,(decoder_hidden,decoder_cell) = decoder(input_tensor = decoder_input,prev_hidden = decoder_hidden,prev_cell = decoder_cell)\n",
    "            else:\n",
    "                decoder_output,decoder_hidden = decoder(decoder_input,decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            \n",
    "            if di<target_length:\n",
    "                loss += criterion(decoder_output,target_tensor[di])\n",
    "                \n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_word += '#'\n",
    "                break\n",
    "            else :\n",
    "                decoded_word += output_lang.index2chr[topi.item()]\n",
    "                \n",
    "            decoder_input = topi.squeeze().detach()\n",
    "        \n",
    "        return decoded_word,loss\n",
    "            \n",
    "def evaluateRandomly(encoder,decoder,n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(train_pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_word = evaluate(encoder, decoder, pair[0])\n",
    "        print('<', output_word)\n",
    "        print('')\n",
    "\n",
    "def eval_acc(encoder,decoder,pairs,criterion = None):\n",
    "    count = 0\n",
    "    tot_loss = 0\n",
    "    for pair in pairs:\n",
    "        pred_word,loss = evaluate(encoder,decoder,pair[0],target=pair[1],criterion=criterion)\n",
    "        tot_loss += loss\n",
    "        if pred_word[:-1] == pair[1] : count += 1\n",
    "    return float(count/len(pairs)),tot_loss\n",
    "\n",
    "def run(encoder,decoder,encoder_optimizer,decoder_optimizer,criterion,epochs):\n",
    "    for epoch in range(epochs): \n",
    "        train_loss = trainIters(encoder=encoder,decoder=decoder,encoder_optimizer=encoder_optimizer,decoder_optimizer=decoder_optimizer,criterion = criterion)\n",
    "        train_acc,_ = eval_acc(encoder,decoder,random.sample(train_pairs,1000),criterion)\n",
    "        valid_acc,valid_loss = eval_acc(encoder,decoder,valid_pairs,criterion)\n",
    "        print(\"Epoch : %d, Training Loss : %f, Training Accuracy : %f, Validation Loss = %f, Validation Accuracy : %f\" % (epoch+1,train_loss,train_acc,valid_loss,valid_acc)) \n",
    "        wandb.log({ \"training_accuracy\" : train_acc,\n",
    "                    \"validation_accuracy\" : valid_acc,\n",
    "                    \"training_loss\" : train_loss,\n",
    "                    \"validation_loss\" : valid_loss,\n",
    "                    \"epoch\" : epoch+1})\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_size = input_lang.n_chrs\n",
    "# output_size = output_lang.n_chrs\n",
    "# embedding_size = 64\n",
    "# hidden_layer_size = 64\n",
    "# num_layers = 2\n",
    "# cell_type = 'LSTM'\n",
    "# dropout_prob = 0.05\n",
    "# learning_rate = 0.01\n",
    "# bidirectional = False\n",
    "# epochs = 15\n",
    "\n",
    "# encoder = EncoderRNN(input_size=input_size,embedding_size=embedding_size,hidden_layer_size=hidden_layer_size,num_encoder_layers=num_layers,cell_type=cell_type,dropout_prob=dropout_prob,bidirectional=bidirectional).to(device)\n",
    "# decoder = DecoderRNN(output_size=output_size,embedding_size=embedding_size,hidden_layer_size=hidden_layer_size,num_decoder_layers=num_layers,cell_type=cell_type,dropout_prob=dropout_prob,bidirectional=bidirectional).to(device)\n",
    "# encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "# decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "# criterion = nn.NLLLoss()\n",
    "# for epoch in range(epochs): \n",
    "#     train_loss = trainIters(encoder=encoder,decoder=decoder,encoder_optimizer=encoder_optimizer,decoder_optimizer=decoder_optimizer,criterion = criterion)\n",
    "#     train_acc,_ = eval_acc(encoder,decoder,train_pairs[0:10],criterion)\n",
    "#     valid_acc,valid_loss = eval_acc(encoder,decoder,valid_pairs[0:10],criterion)\n",
    "#     print(\"Epoch : %d, Training Loss : %f, Trainging Accuracy : %f, Validation Loss = %f, Validation Accuracy : %f\" % (epoch+1,train_loss,train_acc,valid_loss,valid_acc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    config_defaults = {\n",
    "        \"embedding_size\" : 256,\n",
    "        \"hidden_layer_size\" : 256,\n",
    "        \"num_layers\" : 2,\n",
    "        \"cell_type\" : 'GRU',\n",
    "        \"dropout_prob\" : 0.1,\n",
    "        \"learning_rate\" : 0.001,\n",
    "        \"bidirectional\" : False,\n",
    "        \"epochs\" : 10,\n",
    "    }\n",
    "    \n",
    "    wandb.init(config=config_defaults,dir='/home/arunesh/.local/lib/python3.8/site-packages')\n",
    "    config = wandb.config\n",
    "    input_size = input_lang.n_chrs\n",
    "    output_size = output_lang.n_chrs\n",
    "    embedding_size = config.embedding_size\n",
    "    hidden_layer_size = config.hidden_layer_size\n",
    "    num_layers = config.num_layers\n",
    "    cell_type = config.cell_type\n",
    "    dropout_prob = config.dropout_prob\n",
    "    learning_rate = config.learning_rate\n",
    "    bidirectional = config.bidirectional\n",
    "    epochs = config.epochs\n",
    "    encoder = EncoderRNN(input_size=input_size,embedding_size=embedding_size,hidden_layer_size=hidden_layer_size,num_encoder_layers=num_layers,cell_type=cell_type,dropout_prob=dropout_prob,bidirectional=bidirectional).to(device)\n",
    "    decoder = DecoderRNN(output_size=output_size,embedding_size=embedding_size,hidden_layer_size=hidden_layer_size,num_decoder_layers=num_layers,cell_type=cell_type,dropout_prob=dropout_prob,bidirectional=bidirectional).to(device)\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "    print(\"started\")\n",
    "    run(encoder,decoder,encoder_optimizer,decoder_optimizer,criterion,epochs)\n",
    "               \n",
    "    run_name = \"es_{}_hl_{}_nl_{}_ct_{}_dp_{}_lr_{}_bi_{}_ep_{}\".format(embedding_size,hidden_layer_size,num_layers,cell_type,dropout_prob,learning_rate,bidirectional,epochs)\n",
    "    wandb.run.name = run_name\n",
    "    wandb.run.save()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    # \"name\" : \"assignment_sweeps\",\n",
    "    \"method\" : \"bayes\",\n",
    "    \"metric\" :{\n",
    "        \"name\" : \"validation_accuracy\",\n",
    "        \"goal\" : \"maximize\"\n",
    "    },\n",
    "    \"parameters\" : {\n",
    "        \"embedding_size\" : {\n",
    "            \"values\" : [128,256]\n",
    "        },\n",
    "        \"num_layers\" : {\n",
    "            \"values\" : [2,3,4]\n",
    "        },\n",
    "        \"hidden_layer_size\" : {\n",
    "            \"values\" : [128,256]\n",
    "        },\n",
    "        \"learning_rate\" : {\n",
    "            \"values\" : [5e-3,1e-3]\n",
    "        },\n",
    "        \"cell_type\" : {\n",
    "           \"values\" : ['LSTM', 'GRU'] \n",
    "        },\n",
    "        \"dropout\" : {\n",
    "            \"values\" : [0.1,0.2]\n",
    "        },\n",
    "        \"bidirectional\": {\n",
    "            \"values\" : [True,False]\n",
    "        },\n",
    "        \"epochs\": {\n",
    "            \"values\" : [10]\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 1nprjcog\n",
      "Sweep URL: https://wandb.ai/cs20b009/Assignment_3/sweeps/1nprjcog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Agent Starting Run: nc50zsd4 with config:\n",
      "wandb: \tbidirectional: False\n",
      "wandb: \tcell_type: LSTM\n",
      "wandb: \tdropout: 0.2\n",
      "wandb: \tembedding_size: 128\n",
      "wandb: \tepochs: 10\n",
      "wandb: \thidden_layer_size: 128\n",
      "wandb: \tlearning_rate: 0.005\n",
      "wandb: \tnum_layers: 2\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "2023-05-17 18:48:54.249631: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-17 18:48:54.534558: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-17 18:48:54.534595: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-05-17 18:48:56.593991: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-17 18:48:56.594182: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-17 18:48:56.594207: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/arunesh/.local/lib/python3.8/site-packages/wandb/run-20230517_184851-nc50zsd4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/cs20b009/Assignment_3/runs/nc50zsd4\" target=\"_blank\">light-sweep-1</a></strong> to <a href=\"https://wandb.ai/cs20b009/Assignment_3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/cs20b009/Assignment_3/sweeps/1nprjcog\" target=\"_blank\">https://wandb.ai/cs20b009/Assignment_3/sweeps/1nprjcog</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started\n",
      "0 done\n",
      "200 done\n",
      "400 done\n",
      "600 done\n",
      "800 done\n",
      "1000 done\n",
      "1200 done\n",
      "1400 done\n",
      "1600 done\n",
      "1800 done\n",
      "2000 done\n",
      "2200 done\n",
      "2400 done\n",
      "2600 done\n",
      "2800 done\n",
      "3000 done\n",
      "3200 done\n",
      "3400 done\n",
      "3600 done\n",
      "3800 done\n",
      "4000 done\n",
      "4200 done\n",
      "4400 done\n",
      "4600 done\n",
      "4800 done\n",
      "5000 done\n",
      "5200 done\n",
      "5400 done\n",
      "5600 done\n",
      "5800 done\n",
      "6000 done\n",
      "6200 done\n",
      "6400 done\n",
      "6600 done\n",
      "6800 done\n",
      "7000 done\n",
      "7200 done\n",
      "7400 done\n",
      "7600 done\n",
      "7800 done\n",
      "8000 done\n",
      "8200 done\n",
      "8400 done\n",
      "8600 done\n",
      "8800 done\n",
      "9000 done\n",
      "9200 done\n",
      "9400 done\n",
      "9600 done\n",
      "9800 done\n",
      "10000 done\n",
      "Epoch : 1, Training Loss : 1.916255, Training Accuracy : 0.045000, Validation Loss = 40871.492188, Validation Accuracy : 0.072086\n",
      "0 done\n",
      "200 done\n",
      "400 done\n",
      "600 done\n",
      "800 done\n",
      "1000 done\n",
      "1200 done\n",
      "1400 done\n",
      "1600 done\n",
      "1800 done\n",
      "2000 done\n",
      "2200 done\n",
      "2400 done\n",
      "2600 done\n",
      "2800 done\n",
      "3000 done\n",
      "3200 done\n",
      "3400 done\n",
      "3600 done\n",
      "3800 done\n",
      "4000 done\n",
      "4200 done\n",
      "4400 done\n",
      "4600 done\n",
      "4800 done\n",
      "5000 done\n",
      "5200 done\n",
      "5400 done\n",
      "5600 done\n",
      "5800 done\n",
      "6000 done\n",
      "6200 done\n",
      "6400 done\n",
      "6600 done\n",
      "6800 done\n",
      "7000 done\n",
      "7200 done\n",
      "7400 done\n",
      "7600 done\n",
      "7800 done\n",
      "8000 done\n",
      "8200 done\n",
      "8400 done\n",
      "8600 done\n",
      "8800 done\n",
      "9000 done\n",
      "9200 done\n",
      "9400 done\n",
      "9600 done\n",
      "9800 done\n",
      "10000 done\n",
      "Epoch : 2, Training Loss : 1.337947, Training Accuracy : 0.156000, Validation Loss = 35375.968750, Validation Accuracy : 0.140798\n",
      "0 done\n",
      "200 done\n",
      "400 done\n",
      "600 done\n",
      "800 done\n",
      "1000 done\n",
      "1200 done\n",
      "1400 done\n",
      "1600 done\n",
      "1800 done\n",
      "2000 done\n",
      "2200 done\n",
      "2400 done\n",
      "2600 done\n",
      "2800 done\n",
      "3000 done\n",
      "3200 done\n",
      "3400 done\n",
      "3600 done\n",
      "3800 done\n",
      "4000 done\n",
      "4200 done\n",
      "4400 done\n",
      "4600 done\n",
      "4800 done\n",
      "5000 done\n",
      "5200 done\n",
      "5400 done\n",
      "5600 done\n",
      "5800 done\n",
      "6000 done\n",
      "6200 done\n",
      "6400 done\n",
      "6600 done\n",
      "6800 done\n",
      "7000 done\n",
      "7200 done\n",
      "7400 done\n",
      "7600 done\n",
      "7800 done\n",
      "8000 done\n",
      "8200 done\n",
      "8400 done\n",
      "8600 done\n",
      "8800 done\n",
      "9000 done\n",
      "9200 done\n",
      "9400 done\n",
      "9600 done\n",
      "9800 done\n",
      "10000 done\n",
      "Epoch : 3, Training Loss : 1.157109, Training Accuracy : 0.225000, Validation Loss = 32684.060547, Validation Accuracy : 0.191411\n",
      "0 done\n",
      "200 done\n",
      "400 done\n",
      "600 done\n",
      "800 done\n",
      "1000 done\n",
      "1200 done\n",
      "1400 done\n",
      "1600 done\n",
      "1800 done\n",
      "2000 done\n",
      "2200 done\n",
      "2400 done\n",
      "2600 done\n",
      "2800 done\n",
      "3000 done\n",
      "3200 done\n",
      "3400 done\n",
      "3600 done\n",
      "3800 done\n",
      "4000 done\n",
      "4200 done\n",
      "4400 done\n",
      "4600 done\n",
      "4800 done\n",
      "5000 done\n",
      "5200 done\n",
      "5400 done\n",
      "5600 done\n",
      "5800 done\n",
      "6000 done\n",
      "6200 done\n",
      "6400 done\n",
      "6600 done\n",
      "6800 done\n",
      "7000 done\n",
      "7200 done\n",
      "7400 done\n",
      "7600 done\n",
      "7800 done\n",
      "8000 done\n",
      "8200 done\n",
      "8400 done\n",
      "8600 done\n",
      "8800 done\n",
      "9000 done\n",
      "9200 done\n",
      "9400 done\n",
      "9600 done\n",
      "9800 done\n",
      "10000 done\n",
      "Epoch : 4, Training Loss : 1.063774, Training Accuracy : 0.220000, Validation Loss = 30812.605469, Validation Accuracy : 0.211963\n",
      "0 done\n",
      "200 done\n",
      "400 done\n",
      "600 done\n",
      "800 done\n",
      "1000 done\n",
      "1200 done\n",
      "1400 done\n",
      "1600 done\n",
      "1800 done\n",
      "2000 done\n",
      "2200 done\n",
      "2400 done\n",
      "2600 done\n",
      "2800 done\n",
      "3000 done\n",
      "3200 done\n",
      "3400 done\n",
      "3600 done\n",
      "3800 done\n",
      "4000 done\n",
      "4200 done\n",
      "4400 done\n",
      "4600 done\n",
      "4800 done\n",
      "5000 done\n",
      "5200 done\n",
      "5400 done\n",
      "5600 done\n",
      "5800 done\n",
      "6000 done\n",
      "6200 done\n",
      "6400 done\n",
      "6600 done\n",
      "6800 done\n",
      "7000 done\n",
      "7200 done\n",
      "7400 done\n",
      "7600 done\n",
      "7800 done\n",
      "8000 done\n",
      "8200 done\n",
      "8400 done\n",
      "8600 done\n",
      "8800 done\n",
      "9000 done\n",
      "9200 done\n",
      "9400 done\n",
      "9600 done\n",
      "9800 done\n",
      "10000 done\n",
      "Epoch : 5, Training Loss : 0.992997, Training Accuracy : 0.268000, Validation Loss = 30179.449219, Validation Accuracy : 0.229448\n",
      "0 done\n",
      "200 done\n",
      "400 done\n",
      "600 done\n",
      "800 done\n",
      "1000 done\n",
      "1200 done\n",
      "1400 done\n",
      "1600 done\n",
      "1800 done\n",
      "2000 done\n",
      "2200 done\n",
      "2400 done\n",
      "2600 done\n",
      "2800 done\n",
      "3000 done\n",
      "3200 done\n",
      "3400 done\n",
      "3600 done\n",
      "3800 done\n",
      "4000 done\n",
      "4200 done\n",
      "4400 done\n",
      "4600 done\n",
      "4800 done\n",
      "5000 done\n",
      "5200 done\n",
      "5400 done\n",
      "5600 done\n",
      "5800 done\n",
      "6000 done\n",
      "6200 done\n",
      "6400 done\n",
      "6600 done\n",
      "6800 done\n",
      "7000 done\n",
      "7200 done\n",
      "7400 done\n",
      "7600 done\n",
      "7800 done\n",
      "8000 done\n",
      "8200 done\n",
      "8400 done\n",
      "8600 done\n",
      "8800 done\n",
      "9000 done\n",
      "9200 done\n",
      "9400 done\n",
      "9600 done\n",
      "9800 done\n",
      "10000 done\n",
      "Epoch : 6, Training Loss : 0.940111, Training Accuracy : 0.289000, Validation Loss = 28942.390625, Validation Accuracy : 0.273313\n",
      "0 done\n",
      "200 done\n",
      "400 done\n",
      "600 done\n",
      "800 done\n",
      "1000 done\n",
      "1200 done\n",
      "1400 done\n",
      "1600 done\n",
      "1800 done\n",
      "2000 done\n",
      "2200 done\n",
      "2400 done\n",
      "2600 done\n",
      "2800 done\n",
      "3000 done\n",
      "3200 done\n",
      "3400 done\n",
      "3600 done\n",
      "3800 done\n",
      "4000 done\n",
      "4200 done\n",
      "4400 done\n",
      "4600 done\n",
      "4800 done\n",
      "5000 done\n",
      "5200 done\n",
      "5400 done\n",
      "5600 done\n",
      "5800 done\n",
      "6000 done\n",
      "6200 done\n",
      "6400 done\n",
      "6600 done\n",
      "6800 done\n",
      "7000 done\n",
      "7200 done\n",
      "7400 done\n",
      "7600 done\n",
      "7800 done\n",
      "8000 done\n",
      "8200 done\n",
      "8400 done\n",
      "8600 done\n",
      "8800 done\n",
      "9000 done\n",
      "9200 done\n",
      "9400 done\n",
      "9600 done\n",
      "9800 done\n",
      "10000 done\n",
      "Epoch : 7, Training Loss : 0.897920, Training Accuracy : 0.303000, Validation Loss = 28482.349609, Validation Accuracy : 0.282822\n",
      "0 done\n",
      "200 done\n",
      "400 done\n",
      "600 done\n",
      "800 done\n",
      "1000 done\n",
      "1200 done\n",
      "1400 done\n",
      "1600 done\n",
      "1800 done\n",
      "2000 done\n",
      "2200 done\n",
      "2400 done\n",
      "2600 done\n",
      "2800 done\n",
      "3000 done\n",
      "3200 done\n",
      "3400 done\n",
      "3600 done\n",
      "3800 done\n",
      "4000 done\n",
      "4200 done\n",
      "4400 done\n",
      "4600 done\n",
      "4800 done\n",
      "5000 done\n",
      "5200 done\n",
      "5400 done\n",
      "5600 done\n",
      "5800 done\n",
      "6000 done\n",
      "6200 done\n",
      "6400 done\n",
      "6600 done\n",
      "6800 done\n",
      "7000 done\n",
      "7200 done\n",
      "7400 done\n",
      "7600 done\n",
      "7800 done\n",
      "8000 done\n",
      "8200 done\n",
      "8400 done\n",
      "8600 done\n",
      "8800 done\n",
      "9000 done\n",
      "9200 done\n",
      "9400 done\n",
      "9600 done\n",
      "9800 done\n",
      "10000 done\n",
      "Epoch : 8, Training Loss : 0.882965, Training Accuracy : 0.337000, Validation Loss = 29225.914062, Validation Accuracy : 0.273006\n",
      "0 done\n",
      "200 done\n",
      "400 done\n",
      "600 done\n",
      "800 done\n",
      "1000 done\n",
      "1200 done\n",
      "1400 done\n",
      "1600 done\n",
      "1800 done\n",
      "2000 done\n",
      "2200 done\n",
      "2400 done\n",
      "2600 done\n",
      "2800 done\n",
      "3000 done\n",
      "3200 done\n",
      "3400 done\n",
      "3600 done\n",
      "3800 done\n",
      "4000 done\n",
      "4200 done\n",
      "4400 done\n",
      "4600 done\n",
      "4800 done\n",
      "5000 done\n",
      "5200 done\n",
      "5400 done\n",
      "5600 done\n",
      "5800 done\n",
      "6000 done\n",
      "6200 done\n",
      "6400 done\n",
      "6600 done\n",
      "6800 done\n",
      "7000 done\n",
      "7200 done\n",
      "7400 done\n",
      "7600 done\n",
      "7800 done\n",
      "8000 done\n",
      "8200 done\n",
      "8400 done\n",
      "8600 done\n",
      "8800 done\n",
      "9000 done\n",
      "9200 done\n",
      "9400 done\n",
      "9600 done\n",
      "9800 done\n",
      "10000 done\n",
      "Epoch : 9, Training Loss : 0.865871, Training Accuracy : 0.320000, Validation Loss = 29834.753906, Validation Accuracy : 0.269018\n",
      "0 done\n",
      "200 done\n",
      "400 done\n",
      "600 done\n",
      "800 done\n",
      "1000 done\n",
      "1200 done\n",
      "1400 done\n",
      "1600 done\n",
      "1800 done\n",
      "2000 done\n",
      "2200 done\n",
      "2400 done\n",
      "2600 done\n",
      "2800 done\n",
      "3000 done\n",
      "3200 done\n",
      "3400 done\n",
      "3600 done\n",
      "3800 done\n",
      "4000 done\n",
      "4200 done\n",
      "4400 done\n",
      "4600 done\n",
      "4800 done\n",
      "5000 done\n",
      "5200 done\n",
      "5400 done\n",
      "5600 done\n",
      "5800 done\n",
      "6000 done\n",
      "6200 done\n",
      "6400 done\n",
      "6600 done\n",
      "6800 done\n",
      "7000 done\n",
      "7200 done\n",
      "7400 done\n",
      "7600 done\n",
      "7800 done\n",
      "8000 done\n",
      "8200 done\n",
      "8400 done\n",
      "8600 done\n",
      "8800 done\n",
      "9000 done\n",
      "9200 done\n",
      "9400 done\n",
      "9600 done\n",
      "9800 done\n",
      "10000 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 10, Training Loss : 0.882003, Training Accuracy : 0.309000, Validation Loss = 28585.304688, Validation Accuracy : 0.282209\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcebb15bd5ff4142a53916369f733c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>training_accuracy</td><td>▁▄▅▅▆▇▇██▇</td></tr><tr><td>training_loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▃▅▆▆█████</td></tr><tr><td>validation_loss</td><td>█▅▃▂▂▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>training_accuracy</td><td>0.309</td></tr><tr><td>training_loss</td><td>0.882</td></tr><tr><td>validation_accuracy</td><td>0.28221</td></tr><tr><td>validation_loss</td><td>28585.30469</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">light-sweep-1</strong>: <a href=\"https://wandb.ai/cs20b009/Assignment_3/runs/nc50zsd4\" target=\"_blank\">https://wandb.ai/cs20b009/Assignment_3/runs/nc50zsd4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/home/arunesh/.local/lib/python3.8/site-packages/wandb/run-20230517_184851-nc50zsd4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Agent Starting Run: m5n26t4y with config:\n",
      "wandb: \tbidirectional: True\n",
      "wandb: \tcell_type: GRU\n",
      "wandb: \tdropout: 0.1\n",
      "wandb: \tembedding_size: 256\n",
      "wandb: \tepochs: 10\n",
      "wandb: \thidden_layer_size: 128\n",
      "wandb: \tlearning_rate: 0.001\n",
      "wandb: \tnum_layers: 3\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "2023-05-17 20:44:08.407956: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-17 20:44:08.551785: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-17 20:44:08.551809: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-05-17 20:44:09.257266: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-17 20:44:09.257348: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-17 20:44:09.257360: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/arunesh/.local/lib/python3.8/site-packages/wandb/run-20230517_204406-m5n26t4y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/cs20b009/Assignment_3/runs/m5n26t4y\" target=\"_blank\">graceful-sweep-2</a></strong> to <a href=\"https://wandb.ai/cs20b009/Assignment_3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/cs20b009/Assignment_3/sweeps/1nprjcog\" target=\"_blank\">https://wandb.ai/cs20b009/Assignment_3/sweeps/1nprjcog</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started\n",
      "0 done\n",
      "200 done\n",
      "400 done\n",
      "600 done\n",
      "800 done\n",
      "1000 done\n",
      "1200 done\n",
      "1400 done\n",
      "1600 done\n",
      "1800 done\n",
      "2000 done\n",
      "2200 done\n",
      "2400 done\n",
      "2600 done\n",
      "2800 done\n",
      "3000 done\n",
      "3200 done\n",
      "3400 done\n",
      "3600 done\n",
      "3800 done\n",
      "4000 done\n",
      "4200 done\n",
      "4400 done\n",
      "4600 done\n",
      "4800 done\n",
      "5000 done\n",
      "5200 done\n",
      "5400 done\n",
      "5600 done\n",
      "5800 done\n",
      "6000 done\n",
      "6200 done\n",
      "6400 done\n",
      "6600 done\n",
      "6800 done\n",
      "7000 done\n",
      "7200 done\n",
      "7400 done\n",
      "7600 done\n",
      "7800 done\n",
      "8000 done\n",
      "8200 done\n",
      "8400 done\n",
      "8600 done\n",
      "8800 done\n",
      "9000 done\n",
      "9200 done\n",
      "9400 done\n",
      "9600 done\n",
      "9800 done\n",
      "10000 done\n",
      "Epoch : 1, Training Loss : 1.186135, Training Accuracy : 0.297000, Validation Loss = 25979.517578, Validation Accuracy : 0.306442\n",
      "0 done\n",
      "200 done\n",
      "400 done\n",
      "600 done\n",
      "800 done\n",
      "1000 done\n",
      "1200 done\n",
      "1400 done\n",
      "1600 done\n",
      "1800 done\n",
      "2000 done\n",
      "2200 done\n",
      "2400 done\n",
      "2600 done\n",
      "2800 done\n",
      "3000 done\n",
      "3200 done\n",
      "3400 done\n",
      "3600 done\n",
      "3800 done\n",
      "4000 done\n",
      "4200 done\n",
      "4400 done\n",
      "4600 done\n",
      "4800 done\n",
      "5000 done\n",
      "5200 done\n",
      "5400 done\n",
      "5600 done\n",
      "5800 done\n",
      "6000 done\n",
      "6200 done\n",
      "6400 done\n",
      "6600 done\n",
      "6800 done\n",
      "7000 done\n",
      "7200 done\n",
      "7400 done\n",
      "7600 done\n",
      "7800 done\n",
      "8000 done\n",
      "8200 done\n",
      "8400 done\n",
      "8600 done\n",
      "8800 done\n",
      "9000 done\n",
      "9200 done\n",
      "9400 done\n",
      "9600 done\n",
      "9800 done\n",
      "10000 done\n",
      "Epoch : 2, Training Loss : 0.738676, Training Accuracy : 0.389000, Validation Loss = 22699.361328, Validation Accuracy : 0.375460\n",
      "0 done\n",
      "200 done\n",
      "400 done\n",
      "600 done\n",
      "800 done\n",
      "1000 done\n",
      "1200 done\n",
      "1400 done\n",
      "1600 done\n",
      "1800 done\n",
      "2000 done\n",
      "2200 done\n",
      "2400 done\n",
      "2600 done\n",
      "2800 done\n",
      "3000 done\n",
      "3200 done\n",
      "3400 done\n",
      "3600 done\n",
      "3800 done\n",
      "4000 done\n",
      "4200 done\n",
      "4400 done\n",
      "4600 done\n",
      "4800 done\n",
      "5000 done\n",
      "5200 done\n",
      "5400 done\n",
      "5600 done\n",
      "5800 done\n",
      "6000 done\n",
      "6200 done\n",
      "6400 done\n",
      "6600 done\n",
      "6800 done\n",
      "7000 done\n",
      "7200 done\n",
      "7400 done\n",
      "7600 done\n",
      "7800 done\n",
      "8000 done\n",
      "8200 done\n",
      "8400 done\n",
      "8600 done\n",
      "8800 done\n",
      "9000 done\n",
      "9200 done\n",
      "9400 done\n",
      "9600 done\n",
      "9800 done\n",
      "10000 done\n",
      "Epoch : 3, Training Loss : 0.678680, Training Accuracy : 0.408000, Validation Loss = 22830.798828, Validation Accuracy : 0.383742\n",
      "0 done\n",
      "200 done\n",
      "400 done\n",
      "600 done\n",
      "800 done\n",
      "1000 done\n",
      "1200 done\n",
      "1400 done\n",
      "1600 done\n",
      "1800 done\n",
      "2000 done\n",
      "2200 done\n",
      "2400 done\n",
      "2600 done\n",
      "2800 done\n",
      "3000 done\n",
      "3200 done\n",
      "3400 done\n",
      "3600 done\n",
      "3800 done\n",
      "4000 done\n",
      "4200 done\n",
      "4400 done\n",
      "4600 done\n",
      "4800 done\n",
      "5000 done\n",
      "5200 done\n",
      "5400 done\n",
      "5600 done\n",
      "5800 done\n",
      "6000 done\n",
      "6200 done\n",
      "6400 done\n",
      "6600 done\n",
      "6800 done\n",
      "7000 done\n",
      "7200 done\n",
      "7400 done\n",
      "7600 done\n",
      "7800 done\n",
      "8000 done\n",
      "8200 done\n",
      "8400 done\n",
      "8600 done\n",
      "8800 done\n",
      "9000 done\n",
      "9200 done\n",
      "9400 done\n",
      "9600 done\n",
      "9800 done\n",
      "10000 done\n",
      "Epoch : 4, Training Loss : 0.646610, Training Accuracy : 0.488000, Validation Loss = 21938.410156, Validation Accuracy : 0.422393\n",
      "0 done\n",
      "200 done\n",
      "400 done\n",
      "600 done\n",
      "800 done\n",
      "1000 done\n",
      "1200 done\n",
      "1400 done\n",
      "1600 done\n",
      "1800 done\n",
      "2000 done\n",
      "2200 done\n",
      "2400 done\n",
      "2600 done\n",
      "2800 done\n",
      "3000 done\n",
      "3200 done\n",
      "3400 done\n",
      "3600 done\n",
      "3800 done\n",
      "4000 done\n",
      "4200 done\n",
      "4400 done\n",
      "4600 done\n",
      "4800 done\n",
      "5000 done\n",
      "5200 done\n",
      "5400 done\n",
      "5600 done\n",
      "5800 done\n",
      "6000 done\n",
      "6200 done\n",
      "6400 done\n",
      "6600 done\n",
      "6800 done\n",
      "7000 done\n",
      "7200 done\n",
      "7400 done\n",
      "7600 done\n",
      "7800 done\n",
      "8000 done\n",
      "8200 done\n",
      "8400 done\n",
      "8600 done\n",
      "8800 done\n",
      "9000 done\n",
      "9200 done\n",
      "9400 done\n",
      "9600 done\n",
      "9800 done\n",
      "10000 done\n",
      "Epoch : 5, Training Loss : 0.637350, Training Accuracy : 0.448000, Validation Loss = 22729.300781, Validation Accuracy : 0.397853\n",
      "0 done\n",
      "200 done\n",
      "400 done\n",
      "600 done\n",
      "800 done\n",
      "1000 done\n",
      "1200 done\n",
      "1400 done\n",
      "1600 done\n",
      "1800 done\n",
      "2000 done\n",
      "2200 done\n",
      "2400 done\n",
      "2600 done\n",
      "2800 done\n",
      "3000 done\n",
      "3200 done\n",
      "3400 done\n",
      "3600 done\n",
      "3800 done\n",
      "4000 done\n",
      "4200 done\n",
      "4400 done\n",
      "4600 done\n",
      "4800 done\n",
      "5000 done\n",
      "5200 done\n",
      "5400 done\n",
      "5600 done\n",
      "5800 done\n",
      "6000 done\n",
      "6200 done\n",
      "6400 done\n",
      "6600 done\n",
      "6800 done\n",
      "7000 done\n",
      "7200 done\n",
      "7400 done\n",
      "7600 done\n",
      "7800 done\n",
      "8000 done\n",
      "8200 done\n",
      "8400 done\n",
      "8600 done\n",
      "8800 done\n",
      "9000 done\n",
      "9200 done\n",
      "9400 done\n",
      "9600 done\n",
      "9800 done\n",
      "10000 done\n",
      "Epoch : 6, Training Loss : 0.652829, Training Accuracy : 0.406000, Validation Loss = 22206.070312, Validation Accuracy : 0.397546\n",
      "0 done\n",
      "200 done\n",
      "400 done\n",
      "600 done\n",
      "800 done\n",
      "1000 done\n",
      "1200 done\n",
      "1400 done\n",
      "1600 done\n",
      "1800 done\n",
      "2000 done\n",
      "2200 done\n",
      "2400 done\n",
      "2600 done\n",
      "2800 done\n",
      "3000 done\n",
      "3200 done\n",
      "3400 done\n",
      "3600 done\n",
      "3800 done\n",
      "4000 done\n",
      "4200 done\n",
      "4400 done\n",
      "4600 done\n",
      "4800 done\n",
      "5000 done\n",
      "5200 done\n",
      "5400 done\n",
      "5600 done\n",
      "5800 done\n",
      "6000 done\n",
      "6200 done\n",
      "6400 done\n",
      "6600 done\n",
      "6800 done\n",
      "7000 done\n",
      "7200 done\n",
      "7400 done\n",
      "7600 done\n",
      "7800 done\n",
      "8000 done\n",
      "8200 done\n",
      "8400 done\n",
      "8600 done\n",
      "8800 done\n",
      "9000 done\n",
      "9200 done\n",
      "9400 done\n",
      "9600 done\n",
      "9800 done\n",
      "10000 done\n",
      "Epoch : 7, Training Loss : 0.625398, Training Accuracy : 0.388000, Validation Loss = 23381.769531, Validation Accuracy : 0.376074\n",
      "0 done\n",
      "200 done\n",
      "400 done\n",
      "600 done\n",
      "800 done\n",
      "1000 done\n",
      "1200 done\n",
      "1400 done\n",
      "1600 done\n",
      "1800 done\n",
      "2000 done\n",
      "2200 done\n",
      "2400 done\n",
      "2600 done\n",
      "2800 done\n",
      "3000 done\n",
      "3200 done\n",
      "3400 done\n",
      "3600 done\n",
      "3800 done\n",
      "4000 done\n",
      "4200 done\n",
      "4400 done\n",
      "4600 done\n",
      "4800 done\n",
      "5000 done\n",
      "5200 done\n",
      "5400 done\n",
      "5600 done\n",
      "5800 done\n",
      "6000 done\n",
      "6200 done\n",
      "6400 done\n",
      "6600 done\n",
      "6800 done\n",
      "7000 done\n",
      "7200 done\n",
      "7400 done\n",
      "7600 done\n",
      "7800 done\n",
      "8000 done\n",
      "8200 done\n",
      "8400 done\n",
      "8600 done\n",
      "8800 done\n",
      "9000 done\n",
      "9200 done\n",
      "9400 done\n",
      "9600 done\n",
      "9800 done\n",
      "10000 done\n",
      "Epoch : 8, Training Loss : 0.629424, Training Accuracy : 0.421000, Validation Loss = 22354.369141, Validation Accuracy : 0.380675\n",
      "0 done\n",
      "200 done\n",
      "400 done\n",
      "600 done\n",
      "800 done\n",
      "1000 done\n",
      "1200 done\n",
      "1400 done\n",
      "1600 done\n",
      "1800 done\n",
      "2000 done\n",
      "2200 done\n",
      "2400 done\n",
      "2600 done\n",
      "2800 done\n",
      "3000 done\n",
      "3200 done\n",
      "3400 done\n",
      "3600 done\n",
      "3800 done\n",
      "4000 done\n",
      "4200 done\n",
      "4400 done\n",
      "4600 done\n",
      "4800 done\n",
      "5000 done\n",
      "5200 done\n",
      "5400 done\n",
      "5600 done\n",
      "5800 done\n",
      "6000 done\n",
      "6200 done\n",
      "6400 done\n",
      "6600 done\n",
      "6800 done\n",
      "7000 done\n",
      "7200 done\n",
      "7400 done\n",
      "7600 done\n",
      "7800 done\n",
      "8000 done\n",
      "8200 done\n",
      "8400 done\n",
      "8600 done\n",
      "8800 done\n",
      "9000 done\n",
      "9200 done\n",
      "9400 done\n",
      "9600 done\n",
      "9800 done\n",
      "10000 done\n",
      "Epoch : 9, Training Loss : 0.625079, Training Accuracy : 0.405000, Validation Loss = 22071.589844, Validation Accuracy : 0.387117\n",
      "0 done\n",
      "200 done\n",
      "400 done\n",
      "600 done\n",
      "800 done\n",
      "1000 done\n",
      "1200 done\n",
      "1400 done\n",
      "1600 done\n",
      "1800 done\n",
      "2000 done\n",
      "2200 done\n",
      "2400 done\n",
      "2600 done\n",
      "2800 done\n",
      "3000 done\n",
      "3200 done\n",
      "3400 done\n",
      "3600 done\n",
      "3800 done\n",
      "4000 done\n",
      "4200 done\n",
      "4400 done\n",
      "4600 done\n",
      "4800 done\n",
      "5000 done\n",
      "5200 done\n",
      "5400 done\n",
      "5600 done\n",
      "5800 done\n",
      "6000 done\n",
      "6200 done\n",
      "6400 done\n",
      "6600 done\n",
      "6800 done\n",
      "7000 done\n",
      "7200 done\n",
      "7400 done\n",
      "7600 done\n",
      "7800 done\n",
      "8000 done\n",
      "8200 done\n",
      "8400 done\n",
      "8600 done\n",
      "8800 done\n",
      "9000 done\n",
      "9200 done\n",
      "9400 done\n",
      "9600 done\n",
      "9800 done\n",
      "10000 done\n",
      "Epoch : 10, Training Loss : 0.629118, Training Accuracy : 0.422000, Validation Loss = 22421.402344, Validation Accuracy : 0.392331\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe6d2c09f10048dcaebeaf143da66243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>training_accuracy</td><td>▁▄▅█▇▅▄▆▅▆</td></tr><tr><td>training_loss</td><td>█▂▂▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▅▆█▇▇▅▅▆▆</td></tr><tr><td>validation_loss</td><td>█▂▃▁▂▁▄▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>training_accuracy</td><td>0.422</td></tr><tr><td>training_loss</td><td>0.62912</td></tr><tr><td>validation_accuracy</td><td>0.39233</td></tr><tr><td>validation_loss</td><td>22421.40234</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">graceful-sweep-2</strong>: <a href=\"https://wandb.ai/cs20b009/Assignment_3/runs/m5n26t4y\" target=\"_blank\">https://wandb.ai/cs20b009/Assignment_3/runs/m5n26t4y</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/home/arunesh/.local/lib/python3.8/site-packages/wandb/run-20230517_204406-m5n26t4y/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Sweep Agent: Waiting for job.\n",
      "wandb: Job received.\n",
      "wandb: Agent Starting Run: t4wpnzeg with config:\n",
      "wandb: \tbidirectional: False\n",
      "wandb: \tcell_type: GRU\n",
      "wandb: \tdropout: 0.2\n",
      "wandb: \tembedding_size: 256\n",
      "wandb: \tepochs: 10\n",
      "wandb: \thidden_layer_size: 256\n",
      "wandb: \tlearning_rate: 0.005\n",
      "wandb: \tnum_layers: 2\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "2023-05-18 00:22:20.696062: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-18 00:22:20.852762: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-18 00:22:20.852799: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-05-18 00:22:21.474782: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-18 00:22:21.474838: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-18 00:22:21.474846: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/arunesh/.local/lib/python3.8/site-packages/wandb/run-20230518_002219-t4wpnzeg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/cs20b009/Assignment_3/runs/t4wpnzeg\" target=\"_blank\">glowing-sweep-3</a></strong> to <a href=\"https://wandb.ai/cs20b009/Assignment_3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/cs20b009/Assignment_3/sweeps/1nprjcog\" target=\"_blank\">https://wandb.ai/cs20b009/Assignment_3/sweeps/1nprjcog</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started\n",
      "0 done\n",
      "200 done\n",
      "400 done\n",
      "600 done\n",
      "800 done\n",
      "1000 done\n",
      "1200 done\n",
      "1400 done\n",
      "1600 done\n",
      "1800 done\n",
      "2000 done\n",
      "2200 done\n",
      "2400 done\n",
      "2600 done\n",
      "2800 done\n",
      "3000 done\n",
      "3200 done\n",
      "3400 done\n",
      "3600 done\n",
      "3800 done\n",
      "4000 done\n",
      "4200 done\n",
      "4400 done\n",
      "4600 done\n",
      "4800 done\n",
      "5000 done\n",
      "5200 done\n",
      "5400 done\n",
      "5600 done\n",
      "5800 done\n",
      "6000 done\n",
      "6200 done\n",
      "6400 done\n",
      "6600 done\n",
      "6800 done\n",
      "7000 done\n",
      "7200 done\n",
      "7400 done\n",
      "7600 done\n",
      "7800 done\n",
      "8000 done\n",
      "8200 done\n",
      "8400 done\n",
      "8600 done\n",
      "8800 done\n",
      "9000 done\n",
      "9200 done\n",
      "9400 done\n",
      "9600 done\n",
      "9800 done\n",
      "10000 done\n",
      "Epoch : 1, Training Loss : 2.230416, Training Accuracy : 0.008000, Validation Loss = 47054.136719, Validation Accuracy : 0.011350\n",
      "0 done\n",
      "200 done\n",
      "400 done\n",
      "600 done\n",
      "800 done\n",
      "1000 done\n",
      "1200 done\n",
      "1400 done\n",
      "1600 done\n",
      "1800 done\n",
      "2000 done\n",
      "2200 done\n",
      "2400 done\n",
      "2600 done\n",
      "2800 done\n",
      "3000 done\n",
      "3200 done\n",
      "3400 done\n",
      "3600 done\n",
      "3800 done\n",
      "4000 done\n",
      "4200 done\n",
      "4400 done\n",
      "4600 done\n",
      "4800 done\n",
      "5000 done\n",
      "5200 done\n",
      "5400 done\n",
      "5600 done\n",
      "5800 done\n",
      "6000 done\n",
      "6200 done\n",
      "6400 done\n",
      "6600 done\n",
      "6800 done\n",
      "7000 done\n",
      "7200 done\n",
      "7400 done\n",
      "7600 done\n",
      "7800 done\n",
      "8000 done\n",
      "8200 done\n",
      "8400 done\n",
      "8600 done\n",
      "8800 done\n",
      "9000 done\n",
      "9200 done\n",
      "9400 done\n",
      "9600 done\n",
      "9800 done\n",
      "10000 done\n",
      "Epoch : 2, Training Loss : 1.934949, Training Accuracy : 0.004000, Validation Loss = 51598.593750, Validation Accuracy : 0.010123\n",
      "0 done\n",
      "200 done\n",
      "400 done\n",
      "600 done\n",
      "800 done\n",
      "1000 done\n",
      "1200 done\n",
      "1400 done\n",
      "1600 done\n",
      "1800 done\n",
      "2000 done\n",
      "2200 done\n",
      "2400 done\n",
      "2600 done\n",
      "2800 done\n",
      "3000 done\n",
      "3200 done\n",
      "3400 done\n",
      "3600 done\n",
      "3800 done\n",
      "4000 done\n",
      "4200 done\n",
      "4400 done\n",
      "4600 done\n",
      "4800 done\n",
      "5000 done\n",
      "5200 done\n",
      "5400 done\n",
      "5600 done\n",
      "5800 done\n",
      "6000 done\n",
      "6200 done\n",
      "6400 done\n",
      "6600 done\n",
      "6800 done\n",
      "7000 done\n",
      "7200 done\n",
      "7400 done\n",
      "7600 done\n",
      "7800 done\n",
      "8000 done\n",
      "8200 done\n",
      "8400 done\n",
      "8600 done\n",
      "8800 done\n",
      "9000 done\n",
      "9200 done\n",
      "9400 done\n",
      "9600 done\n",
      "9800 done\n",
      "10000 done\n",
      "Epoch : 3, Training Loss : 1.894556, Training Accuracy : 0.002000, Validation Loss = 46943.488281, Validation Accuracy : 0.015337\n",
      "0 done\n",
      "200 done\n",
      "400 done\n",
      "600 done\n",
      "800 done\n",
      "1000 done\n",
      "1200 done\n",
      "1400 done\n",
      "1600 done\n",
      "1800 done\n",
      "2000 done\n",
      "2200 done\n",
      "2400 done\n",
      "2600 done\n",
      "2800 done\n",
      "3000 done\n",
      "3200 done\n",
      "3400 done\n",
      "3600 done\n",
      "3800 done\n",
      "4000 done\n",
      "4200 done\n",
      "4400 done\n",
      "4600 done\n",
      "4800 done\n",
      "5000 done\n",
      "5200 done\n",
      "5400 done\n",
      "5600 done\n",
      "5800 done\n",
      "6000 done\n",
      "6200 done\n",
      "6400 done\n",
      "6600 done\n",
      "6800 done\n",
      "7000 done\n",
      "7200 done\n",
      "7400 done\n",
      "7600 done\n",
      "7800 done\n",
      "8000 done\n",
      "8200 done\n",
      "8400 done\n",
      "8600 done\n",
      "8800 done\n",
      "9000 done\n",
      "9200 done\n",
      "9400 done\n",
      "9600 done\n",
      "9800 done\n",
      "10000 done\n",
      "Epoch : 4, Training Loss : 1.846622, Training Accuracy : 0.006000, Validation Loss = 52120.234375, Validation Accuracy : 0.012577\n",
      "0 done\n",
      "200 done\n",
      "400 done\n",
      "600 done\n",
      "800 done\n",
      "1000 done\n",
      "1200 done\n",
      "1400 done\n",
      "1600 done\n",
      "1800 done\n",
      "2000 done\n",
      "2200 done\n",
      "2400 done\n",
      "2600 done\n",
      "2800 done\n",
      "3000 done\n",
      "3200 done\n",
      "3400 done\n",
      "3600 done\n",
      "3800 done\n",
      "4000 done\n",
      "4200 done\n",
      "4400 done\n",
      "4600 done\n",
      "4800 done\n",
      "5000 done\n",
      "5200 done\n",
      "5400 done\n",
      "5600 done\n",
      "5800 done\n",
      "6000 done\n",
      "6200 done\n",
      "6400 done\n",
      "6600 done\n",
      "6800 done\n",
      "7000 done\n",
      "7200 done\n",
      "7400 done\n",
      "7600 done\n",
      "7800 done\n",
      "8000 done\n",
      "8200 done\n",
      "8400 done\n",
      "8600 done\n",
      "8800 done\n",
      "9000 done\n",
      "9200 done\n",
      "9400 done\n",
      "9600 done\n",
      "9800 done\n",
      "10000 done\n",
      "Epoch : 5, Training Loss : 1.852302, Training Accuracy : 0.000000, Validation Loss = 61624.082031, Validation Accuracy : 0.000920\n",
      "0 done\n",
      "200 done\n",
      "400 done\n",
      "600 done\n",
      "800 done\n",
      "1000 done\n",
      "1200 done\n",
      "1400 done\n",
      "1600 done\n",
      "1800 done\n",
      "2000 done\n",
      "2200 done\n",
      "2400 done\n",
      "2600 done\n",
      "2800 done\n",
      "3000 done\n",
      "3200 done\n",
      "3400 done\n",
      "3600 done\n",
      "3800 done\n",
      "4000 done\n",
      "4200 done\n",
      "4400 done\n",
      "4600 done\n",
      "4800 done\n",
      "5000 done\n",
      "5200 done\n",
      "5400 done\n",
      "5600 done\n",
      "5800 done\n",
      "6000 done\n",
      "6200 done\n",
      "6400 done\n",
      "6600 done\n",
      "6800 done\n",
      "7000 done\n",
      "7200 done\n",
      "7400 done\n",
      "7600 done\n",
      "7800 done\n",
      "8000 done\n",
      "8200 done\n",
      "8400 done\n",
      "8600 done\n",
      "8800 done\n",
      "9000 done\n",
      "9200 done\n",
      "9400 done\n",
      "9600 done\n",
      "9800 done\n",
      "10000 done\n",
      "Epoch : 6, Training Loss : 1.841327, Training Accuracy : 0.005000, Validation Loss = 51407.953125, Validation Accuracy : 0.005215\n",
      "0 done\n",
      "200 done\n",
      "400 done\n",
      "600 done\n",
      "800 done\n",
      "1000 done\n",
      "1200 done\n",
      "1400 done\n",
      "1600 done\n",
      "1800 done\n",
      "2000 done\n",
      "2200 done\n",
      "2400 done\n",
      "2600 done\n",
      "2800 done\n",
      "3000 done\n",
      "3200 done\n",
      "3400 done\n",
      "3600 done\n",
      "3800 done\n",
      "4000 done\n",
      "4200 done\n",
      "4400 done\n",
      "4600 done\n",
      "4800 done\n",
      "5000 done\n",
      "5200 done\n",
      "5400 done\n",
      "5600 done\n",
      "5800 done\n",
      "6000 done\n",
      "6200 done\n",
      "6400 done\n",
      "6600 done\n",
      "6800 done\n",
      "7000 done\n",
      "7200 done\n",
      "7400 done\n",
      "7600 done\n",
      "7800 done\n",
      "8000 done\n",
      "8200 done\n",
      "8400 done\n",
      "8600 done\n",
      "8800 done\n",
      "9000 done\n",
      "9200 done\n",
      "9400 done\n",
      "9600 done\n",
      "9800 done\n",
      "10000 done\n",
      "Epoch : 7, Training Loss : 1.792482, Training Accuracy : 0.004000, Validation Loss = 44174.097656, Validation Accuracy : 0.014110\n",
      "0 done\n",
      "200 done\n",
      "400 done\n",
      "600 done\n",
      "800 done\n",
      "1000 done\n",
      "1200 done\n",
      "1400 done\n",
      "1600 done\n",
      "1800 done\n",
      "2000 done\n",
      "2200 done\n",
      "2400 done\n",
      "2600 done\n",
      "2800 done\n",
      "3000 done\n",
      "3200 done\n",
      "3400 done\n",
      "3600 done\n",
      "3800 done\n",
      "4000 done\n",
      "4200 done\n",
      "4400 done\n",
      "4600 done\n",
      "4800 done\n",
      "5000 done\n",
      "5200 done\n",
      "5400 done\n",
      "5600 done\n",
      "5800 done\n",
      "6000 done\n",
      "6200 done\n",
      "6400 done\n",
      "6600 done\n",
      "6800 done\n",
      "7000 done\n",
      "7200 done\n",
      "7400 done\n",
      "7600 done\n",
      "7800 done\n",
      "8000 done\n",
      "8200 done\n",
      "8400 done\n",
      "8600 done\n",
      "8800 done\n",
      "9000 done\n",
      "9200 done\n",
      "9400 done\n",
      "9600 done\n",
      "9800 done\n",
      "10000 done\n",
      "Epoch : 8, Training Loss : 1.796060, Training Accuracy : 0.008000, Validation Loss = 44322.539062, Validation Accuracy : 0.016564\n",
      "0 done\n",
      "200 done\n",
      "400 done\n",
      "600 done\n",
      "800 done\n",
      "1000 done\n",
      "1200 done\n",
      "1400 done\n",
      "1600 done\n",
      "1800 done\n",
      "2000 done\n",
      "2200 done\n",
      "2400 done\n",
      "2600 done\n",
      "2800 done\n",
      "3000 done\n",
      "3200 done\n",
      "3400 done\n",
      "3600 done\n",
      "3800 done\n",
      "4000 done\n",
      "4200 done\n",
      "4400 done\n",
      "4600 done\n",
      "4800 done\n",
      "5000 done\n",
      "5200 done\n",
      "5400 done\n",
      "5600 done\n",
      "5800 done\n",
      "6000 done\n",
      "6200 done\n",
      "6400 done\n",
      "6600 done\n",
      "6800 done\n",
      "7000 done\n",
      "7200 done\n",
      "7400 done\n",
      "7600 done\n",
      "7800 done\n",
      "8000 done\n",
      "8200 done\n",
      "8400 done\n",
      "8600 done\n",
      "8800 done\n",
      "9000 done\n",
      "9200 done\n",
      "9400 done\n",
      "9600 done\n",
      "9800 done\n",
      "10000 done\n",
      "Epoch : 9, Training Loss : 1.765449, Training Accuracy : 0.006000, Validation Loss = 50262.320312, Validation Accuracy : 0.007975\n",
      "0 done\n",
      "200 done\n",
      "400 done\n",
      "600 done\n",
      "800 done\n",
      "1000 done\n",
      "1200 done\n",
      "1400 done\n",
      "1600 done\n",
      "1800 done\n",
      "2000 done\n",
      "2200 done\n",
      "2400 done\n",
      "2600 done\n",
      "2800 done\n",
      "3000 done\n",
      "3200 done\n",
      "3400 done\n",
      "3600 done\n",
      "3800 done\n",
      "4000 done\n",
      "4200 done\n",
      "4400 done\n",
      "4600 done\n",
      "4800 done\n",
      "5000 done\n",
      "5200 done\n",
      "5400 done\n",
      "5600 done\n",
      "5800 done\n",
      "6000 done\n",
      "6200 done\n",
      "6400 done\n",
      "6600 done\n",
      "6800 done\n",
      "7000 done\n",
      "7200 done\n",
      "7400 done\n",
      "7600 done\n",
      "7800 done\n",
      "8000 done\n",
      "8200 done\n",
      "8400 done\n",
      "8600 done\n",
      "8800 done\n",
      "9000 done\n",
      "9200 done\n",
      "9400 done\n",
      "9600 done\n",
      "9800 done\n",
      "10000 done\n",
      "Epoch : 10, Training Loss : 1.739392, Training Accuracy : 0.017000, Validation Loss = 40344.074219, Validation Accuracy : 0.016871\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8674321ae56040f9a55d3ca80865f583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>training_accuracy</td><td>▄▃▂▃▁▃▃▄▃█</td></tr><tr><td>training_loss</td><td>█▄▃▃▃▂▂▂▁▁</td></tr><tr><td>validation_accuracy</td><td>▆▅▇▆▁▃▇█▄█</td></tr><tr><td>validation_loss</td><td>▃▅▃▅█▅▂▂▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>training_accuracy</td><td>0.017</td></tr><tr><td>training_loss</td><td>1.73939</td></tr><tr><td>validation_accuracy</td><td>0.01687</td></tr><tr><td>validation_loss</td><td>40344.07422</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">glowing-sweep-3</strong>: <a href=\"https://wandb.ai/cs20b009/Assignment_3/runs/t4wpnzeg\" target=\"_blank\">https://wandb.ai/cs20b009/Assignment_3/runs/t4wpnzeg</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/home/arunesh/.local/lib/python3.8/site-packages/wandb/run-20230518_002219-t4wpnzeg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Agent Starting Run: jqb9ru1n with config:\n",
      "wandb: \tbidirectional: True\n",
      "wandb: \tcell_type: LSTM\n",
      "wandb: \tdropout: 0.1\n",
      "wandb: \tembedding_size: 128\n",
      "wandb: \tepochs: 10\n",
      "wandb: \thidden_layer_size: 256\n",
      "wandb: \tlearning_rate: 0.001\n",
      "wandb: \tnum_layers: 4\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "2023-05-18 02:44:05.165438: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-18 02:44:05.308060: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-18 02:44:05.308082: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-05-18 02:44:05.907297: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-18 02:44:05.907350: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-18 02:44:05.907358: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/arunesh/.local/lib/python3.8/site-packages/wandb/run-20230518_024403-jqb9ru1n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/cs20b009/Assignment_3/runs/jqb9ru1n\" target=\"_blank\">azure-sweep-4</a></strong> to <a href=\"https://wandb.ai/cs20b009/Assignment_3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/cs20b009/Assignment_3/sweeps/1nprjcog\" target=\"_blank\">https://wandb.ai/cs20b009/Assignment_3/sweeps/1nprjcog</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started\n",
      "0 done\n",
      "200 done\n",
      "400 done\n",
      "600 done\n",
      "800 done\n",
      "1000 done\n",
      "1200 done\n",
      "1400 done\n",
      "1600 done\n",
      "1800 done\n",
      "2000 done\n",
      "2200 done\n",
      "2400 done\n",
      "2600 done\n",
      "2800 done\n",
      "3000 done\n",
      "3200 done\n",
      "3400 done\n",
      "3600 done\n",
      "3800 done\n",
      "4000 done\n",
      "4200 done\n",
      "4400 done\n",
      "4600 done\n",
      "4800 done\n",
      "5000 done\n",
      "5200 done\n",
      "5400 done\n",
      "5600 done\n",
      "5800 done\n",
      "6000 done\n",
      "6200 done\n",
      "6400 done\n",
      "6600 done\n",
      "6800 done\n",
      "7000 done\n",
      "7200 done\n",
      "7400 done\n",
      "7600 done\n",
      "7800 done\n",
      "8000 done\n",
      "8200 done\n",
      "8400 done\n",
      "8600 done\n",
      "8800 done\n",
      "9000 done\n",
      "9200 done\n",
      "9400 done\n",
      "9600 done\n",
      "9800 done\n",
      "10000 done\n",
      "Epoch : 1, Training Loss : 2.115265, Training Accuracy : 0.015000, Validation Loss = 43504.816406, Validation Accuracy : 0.022393\n",
      "0 done\n",
      "200 done\n",
      "400 done\n",
      "600 done\n",
      "800 done\n",
      "1000 done\n",
      "1200 done\n",
      "1400 done\n",
      "1600 done\n",
      "1800 done\n",
      "2000 done\n",
      "2200 done\n",
      "2400 done\n",
      "2600 done\n",
      "2800 done\n",
      "3000 done\n",
      "3200 done\n",
      "3400 done\n",
      "3600 done\n",
      "3800 done\n",
      "4000 done\n",
      "4200 done\n",
      "4400 done\n",
      "4600 done\n",
      "4800 done\n",
      "5000 done\n",
      "5200 done\n",
      "5400 done\n",
      "5600 done\n",
      "5800 done\n",
      "6000 done\n",
      "6200 done\n",
      "6400 done\n",
      "6600 done\n",
      "6800 done\n",
      "7000 done\n",
      "7200 done\n",
      "7400 done\n",
      "7600 done\n",
      "7800 done\n",
      "8000 done\n",
      "8200 done\n",
      "8400 done\n",
      "8600 done\n",
      "8800 done\n",
      "9000 done\n",
      "9200 done\n",
      "9400 done\n",
      "9600 done\n",
      "9800 done\n",
      "10000 done\n",
      "Epoch : 2, Training Loss : 1.301633, Training Accuracy : 0.273000, Validation Loss = 28313.718750, Validation Accuracy : 0.255828\n",
      "0 done\n",
      "200 done\n",
      "400 done\n",
      "600 done\n",
      "800 done\n",
      "1000 done\n",
      "1200 done\n",
      "1400 done\n",
      "1600 done\n",
      "1800 done\n",
      "2000 done\n",
      "2200 done\n",
      "2400 done\n",
      "2600 done\n",
      "2800 done\n",
      "3000 done\n",
      "3200 done\n",
      "3400 done\n",
      "3600 done\n",
      "3800 done\n",
      "4000 done\n",
      "4200 done\n",
      "4400 done\n",
      "4600 done\n",
      "4800 done\n",
      "5000 done\n",
      "5200 done\n",
      "5400 done\n",
      "5600 done\n",
      "5800 done\n",
      "6000 done\n",
      "6200 done\n",
      "6400 done\n",
      "6600 done\n",
      "6800 done\n",
      "7000 done\n",
      "7200 done\n",
      "7400 done\n",
      "7600 done\n",
      "7800 done\n",
      "8000 done\n",
      "8200 done\n",
      "8400 done\n",
      "8600 done\n",
      "8800 done\n",
      "9000 done\n",
      "9200 done\n",
      "9400 done\n",
      "9600 done\n",
      "9800 done\n",
      "10000 done\n",
      "Epoch : 3, Training Loss : 0.745098, Training Accuracy : 0.454000, Validation Loss = 21253.925781, Validation Accuracy : 0.421166\n",
      "0 done\n",
      "200 done\n",
      "400 done\n",
      "600 done\n",
      "800 done\n",
      "1000 done\n",
      "1200 done\n",
      "1400 done\n",
      "1600 done\n",
      "1800 done\n",
      "2000 done\n",
      "2200 done\n",
      "2400 done\n",
      "2600 done\n",
      "2800 done\n",
      "3000 done\n",
      "3200 done\n",
      "3400 done\n",
      "3600 done\n",
      "3800 done\n",
      "4000 done\n",
      "4200 done\n",
      "4400 done\n",
      "4600 done\n",
      "4800 done\n",
      "5000 done\n",
      "5200 done\n",
      "5400 done\n",
      "5600 done\n",
      "5800 done\n",
      "6000 done\n",
      "6200 done\n",
      "6400 done\n",
      "6600 done\n",
      "6800 done\n",
      "7000 done\n",
      "7200 done\n",
      "7400 done\n",
      "7600 done\n",
      "7800 done\n",
      "8000 done\n",
      "8200 done\n",
      "8400 done\n",
      "8600 done\n",
      "8800 done\n",
      "9000 done\n",
      "9200 done\n",
      "9400 done\n",
      "9600 done\n",
      "9800 done\n",
      "10000 done\n",
      "Epoch : 4, Training Loss : 0.527289, Training Accuracy : 0.564000, Validation Loss = 19193.898438, Validation Accuracy : 0.482209\n",
      "0 done\n",
      "200 done\n",
      "400 done\n",
      "600 done\n",
      "800 done\n",
      "1000 done\n",
      "1200 done\n",
      "1400 done\n",
      "1600 done\n",
      "1800 done\n",
      "2000 done\n",
      "2200 done\n",
      "2400 done\n",
      "2600 done\n",
      "2800 done\n",
      "3000 done\n",
      "3200 done\n",
      "3400 done\n",
      "3600 done\n",
      "3800 done\n",
      "4000 done\n",
      "4200 done\n",
      "4400 done\n",
      "4600 done\n",
      "4800 done\n",
      "5000 done\n",
      "5200 done\n",
      "5400 done\n",
      "5600 done\n",
      "5800 done\n",
      "6000 done\n",
      "6200 done\n",
      "6400 done\n",
      "6600 done\n",
      "6800 done\n",
      "7000 done\n",
      "7200 done\n",
      "7400 done\n",
      "7600 done\n",
      "7800 done\n",
      "8000 done\n",
      "8200 done\n",
      "8400 done\n",
      "8600 done\n",
      "8800 done\n",
      "9000 done\n",
      "9200 done\n",
      "9400 done\n",
      "9600 done\n",
      "9800 done\n",
      "10000 done\n",
      "Epoch : 5, Training Loss : 0.422502, Training Accuracy : 0.674000, Validation Loss = 16337.249023, Validation Accuracy : 0.556748\n",
      "0 done\n",
      "200 done\n",
      "400 done\n",
      "600 done\n",
      "800 done\n",
      "1000 done\n",
      "1200 done\n",
      "1400 done\n",
      "1600 done\n",
      "1800 done\n",
      "2000 done\n",
      "2200 done\n",
      "2400 done\n",
      "2600 done\n",
      "2800 done\n",
      "3000 done\n",
      "3200 done\n",
      "3400 done\n",
      "3600 done\n",
      "3800 done\n",
      "4000 done\n",
      "4200 done\n",
      "4400 done\n",
      "4600 done\n",
      "4800 done\n",
      "5000 done\n",
      "5200 done\n",
      "5400 done\n",
      "5600 done\n",
      "5800 done\n",
      "6000 done\n",
      "6200 done\n",
      "6400 done\n",
      "6600 done\n",
      "6800 done\n",
      "7000 done\n",
      "7200 done\n",
      "7400 done\n",
      "7600 done\n",
      "7800 done\n",
      "8000 done\n",
      "8200 done\n",
      "8400 done\n",
      "8600 done\n",
      "8800 done\n",
      "9000 done\n",
      "9200 done\n",
      "9400 done\n",
      "9600 done\n",
      "9800 done\n",
      "10000 done\n",
      "Epoch : 6, Training Loss : 0.358150, Training Accuracy : 0.704000, Validation Loss = 16057.837891, Validation Accuracy : 0.573926\n",
      "0 done\n",
      "200 done\n",
      "400 done\n",
      "600 done\n",
      "800 done\n",
      "1000 done\n",
      "1200 done\n",
      "1400 done\n",
      "1600 done\n",
      "1800 done\n",
      "2000 done\n",
      "2200 done\n",
      "2400 done\n",
      "2600 done\n",
      "2800 done\n",
      "3000 done\n",
      "3200 done\n",
      "3400 done\n",
      "3600 done\n",
      "3800 done\n",
      "4000 done\n",
      "4200 done\n",
      "4400 done\n",
      "4600 done\n",
      "4800 done\n",
      "5000 done\n",
      "5200 done\n",
      "5400 done\n",
      "5600 done\n",
      "5800 done\n",
      "6000 done\n",
      "6200 done\n",
      "6400 done\n",
      "6600 done\n",
      "6800 done\n",
      "7000 done\n",
      "7200 done\n",
      "7400 done\n",
      "7600 done\n",
      "7800 done\n",
      "8000 done\n",
      "8200 done\n",
      "8400 done\n",
      "8600 done\n",
      "8800 done\n",
      "9000 done\n",
      "9200 done\n",
      "9400 done\n",
      "9600 done\n",
      "9800 done\n",
      "10000 done\n",
      "Epoch : 7, Training Loss : 0.321371, Training Accuracy : 0.723000, Validation Loss = 15688.349609, Validation Accuracy : 0.595706\n",
      "0 done\n",
      "200 done\n",
      "400 done\n",
      "600 done\n",
      "800 done\n",
      "1000 done\n",
      "1200 done\n",
      "1400 done\n",
      "1600 done\n",
      "1800 done\n",
      "2000 done\n",
      "2200 done\n",
      "2400 done\n",
      "2600 done\n",
      "2800 done\n",
      "3000 done\n",
      "3200 done\n",
      "3400 done\n",
      "3600 done\n",
      "3800 done\n",
      "4000 done\n",
      "4200 done\n",
      "4400 done\n",
      "4600 done\n",
      "4800 done\n",
      "5000 done\n",
      "5200 done\n",
      "5400 done\n",
      "5600 done\n",
      "5800 done\n",
      "6000 done\n",
      "6200 done\n",
      "6400 done\n",
      "6600 done\n",
      "6800 done\n",
      "7000 done\n",
      "7200 done\n",
      "7400 done\n",
      "7600 done\n",
      "7800 done\n",
      "8000 done\n",
      "8200 done\n",
      "8400 done\n",
      "8600 done\n",
      "8800 done\n",
      "9000 done\n",
      "9200 done\n",
      "9400 done\n",
      "9600 done\n",
      "9800 done\n",
      "10000 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Ctrl + C detected. Stopping sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196c42ab460d479cbb18917c26140157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▅▆▇█</td></tr><tr><td>training_accuracy</td><td>▁▄▅▆███</td></tr><tr><td>training_loss</td><td>█▅▃▂▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▄▆▇███</td></tr><tr><td>validation_loss</td><td>█▄▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>7</td></tr><tr><td>training_accuracy</td><td>0.723</td></tr><tr><td>training_loss</td><td>0.32137</td></tr><tr><td>validation_accuracy</td><td>0.59571</td></tr><tr><td>validation_loss</td><td>15688.34961</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">azure-sweep-4</strong>: <a href=\"https://wandb.ai/cs20b009/Assignment_3/runs/jqb9ru1n\" target=\"_blank\">https://wandb.ai/cs20b009/Assignment_3/runs/jqb9ru1n</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/home/arunesh/.local/lib/python3.8/site-packages/wandb/run-20230518_024403-jqb9ru1n/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config,project=\"Assignment_3\")\n",
    "wandb.agent(sweep_id = sweep_id,function = train_model,count = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
